{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc08e11f-70cf-4f62-af15-f86654875f1f",
   "metadata": {},
   "source": [
    "# Writing unit tests\n",
    "<img src='../images/xebia-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "This notebook will walk you through how to write unit tests with `pytest`.\n",
    "\n",
    "The first step is installing `pytest` with `poetry add -G dev pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e7b7d-c571-4997-9c04-48738298bb80",
   "metadata": {},
   "source": [
    "`pytest` will run automatically tests located under the `/test` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b06de8-b75a-40ee-973a-102b1859bad7",
   "metadata": {},
   "source": [
    "```plain\n",
    "├── src/​\n",
    "│  └─ animal_shelter/​\n",
    "│     ├── __init__.py​\n",
    "│     ├── data.py​\n",
    "│     └── features.py​\n",
    "└── tests/​\n",
    "│   ├── test_data.py​\n",
    "│   └── test_features.py​\n",
    "└── pyproject.toml​\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c82140-a306-4499-be39-9efd571a082c",
   "metadata": {},
   "source": [
    "There are multiple *test discovery rules* (https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#test-discovery), which is what pytest to identify which tests to run. \n",
    "\n",
    "In general terms, you should mimic the structure of your package in your `test/` folder, preppending each file with `test_` and each testing function or class with `test_` too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6f315-2b89-40aa-98b3-8f2b8e792eb3",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> The first test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02fadf-bee9-47a6-831c-99c997d9c7ac",
   "metadata": {},
   "source": [
    "For example to test the `conver_camel_case` function from `data.py`, you would create the file `tests/test_data.py` with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be53f9-0e59-436e-b0b4-92b3335432c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animal_shelter import data\n",
    "\n",
    "def test_convert_camel_case():\n",
    "    assert data.convert_camel_case(\"CamelCase\") == \"camel_case\"\n",
    "    assert data.convert_camel_case(\"CamelCASE\") == \"camel_case\"\n",
    "    assert data.convert_camel_case(\"camel-case\") != \"camel_case\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c9116-4748-4536-990c-9a17e949b2d8",
   "metadata": {},
   "source": [
    "**Try running `pytest​` from the virtual environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc64a6-cf25-49ed-9338-f93c2bf42749",
   "metadata": {},
   "source": [
    "The `assert` keyword defines a expression that expects a boolean. If the boolean is `True` the program continues, and if it's `False` it raises an exception, which `pytest` interprests as the test not passing.\n",
    "\n",
    "**Try adding extra `assert` statements.**\n",
    "For example,\n",
    "- What would you expect `conver_camel_case()` to return if the input is already in *snake_case*?\n",
    "- What if the input contains whitespace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00522567-bfcb-4a49-8d73-295c4be346ab",
   "metadata": {},
   "source": [
    "## Other assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43084f-73de-4b17-bd53-031fad4d0322",
   "metadata": {},
   "source": [
    "The `assert` keyword is very flexible and is all the syntaxt you would need to write a lot of tests, but there are some other assertion-like functions that come in handy.\n",
    "\n",
    "For example the `assert_series_equal` from the `pandas.testing` module allows to easily compare that the values of two `pandas.Series` are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2864a-813d-405e-b1a4-f83518c7ff1f",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> The second test\n",
    "\n",
    "Can you add the following test for the `check_has_name` function from `features.py`?\n",
    "\n",
    "Think about in which file you need to add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6358b-d500-44e9-a6d5-3ed2247b7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "from animal_shelter import features\n",
    "\n",
    "def test_check_has_name():\n",
    "    s = pd.Series([\"Ivo\", \"Henk\", \"unknown\"])\n",
    "    result = features.check_has_name(s)\n",
    "    expected = pd.Series([True, True, False])\n",
    "    assert_series_equal(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32696563-80ca-48d6-9f65-b936c02a7a9a",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> The third, fourth.... test\n",
    "\n",
    "Add now at least one unit test for each function in `features.py` using assert_series_equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a1e8f-cc1f-4fab-adcf-a3c1598b725e",
   "metadata": {},
   "source": [
    "### Checking for exceptions\n",
    "\n",
    "Another common kind of assertions you might wanna test for is checking that something doesn't work when you expect it not to.\n",
    "In the simplets case you can use an inequality logical comparator with an `assert` statement (e.g. `assert x != y`).\n",
    "But sometimes you want to check that the calling function actually throws an error.\n",
    "To check for exceptions you can use the `pytest.raises()` context manager.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_for_exceptions():\n",
    "    with pytest.raises(EXCEPTION_TYPE):\n",
    "        function_that_errors()\n",
    "```\n",
    "\n",
    "#### <mark> Exercise: </mark> checking for exceptions \n",
    "\n",
    "- Add a test that checks that `convert_camel_case` throws an exception when called with a value that is not a `str`. Check what kind of exception the function raises in that case.\n",
    "\n",
    "- Can you also add some exception checking to some of the functions in `features.py`?\n",
    "\n",
    "When checking for exceptions is important that the checks are as precise as possible, so that your tests don't pass when any error occurs, but only the ones you were expecting. To do so you can write explicit assertions using the exception object that is produced.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_for_exceptions():\n",
    "    with pytest.raises(EXCEPTION_TYPE) as excepcion:\n",
    "        function_that_errors()\n",
    "    assert \"exception message\" in str(excepcion.value)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd4f7f-61d6-4aaf-abdd-4bffa43c7fb8",
   "metadata": {},
   "source": [
    "### A note on testing\n",
    "\n",
    "It can be hard to see the value of testing when doing this kind of exercises, but the practice of writing *real* tests is very different than trying to test a code base post-hoc. During the development process you usually write multiple attemps of each function you write, and those attempts give you insight into which tests are likelly to be more valuable and catch potential issues in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4367e9c-b456-4625-9d16-8119b17a3c7f",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "Fixtures allow you to define functions that setup elements required by (multiple) tests.\n",
    "\n",
    "For example, if two tests use the same input, you can abstract it away into a fixture. To define fixtures you can decorate a function with the `@pytest.fixture` decorator. When you call a testing function that accepts arguments, `pytest` will check if there are fixtures with the same argument names and automatically pass them to the testing functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0dc7a-9fbb-4633-a42a-6bc9ddf20979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_all_nums(list_of_numbers):\n",
    "    assert all(type(element) is int for element in list_of_numbers)\n",
    "\n",
    "def test_sum(list_of_numbers):\n",
    "    assert sum(list_of_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef069174-2c9e-46ba-8dbd-a8a3dbd60aca",
   "metadata": {},
   "source": [
    "The example above behaves simmilarly as having a single testing function with two `assert` expressions.\n",
    "\n",
    "However, fixtures are very flexible and their utility shines through with more complex usecases.\n",
    "\n",
    "The `@fixture` decorator accepts an argument called `scope` that determines *the lifetime of fixtures*. By default `scope=\"funcion\"`, which means that each function that requires a fixture gets a new copy of the output of the fixture. \n",
    "\n",
    "Changing this parameter is useful if, for example, initializing the inputs required for testing is expensive computationally (e.g. connecting to a database). We can choose to group all tests that require the same input into a `class` and set the fixture `scope=\"class\"`. In this case `list_of_numbers` will be generated only once, and passed to both tests.\n",
    "\n",
    "**Warning:** If one of the tests were to mutate `list_of_numbers`, that mutation would carry to the next tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecef281-20df-4522-98db-b6916c3856c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "class TestListFunctions:\n",
    "    def test_all_nums(self, list_of_numbers):\n",
    "        assert all(type(element) is int for element in list_of_numbers)\n",
    "\n",
    "    def test_sum(self, list_of_numbers):\n",
    "        assert sum(list_of_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90202a3e-e6eb-40a0-8990-6a063f11da45",
   "metadata": {},
   "source": [
    "Another common fixture scope is `\"module\"`, where the fixture object is the same for all testing functions within a module (i.e. `.py` file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af9549-94a3-4e60-b96e-db9b608a23e0",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> Add a fixture\n",
    "\n",
    "On `features.py` there are two functions that generate features from the `sex_upon_outcome` variable of the data.\n",
    "\n",
    "Can you write unit tests for them (or re-use the ones you wrote in the previous exercises) that use a fixture to provide the same *mocked* input data for both tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77ed4-d0f2-440c-9012-94cd2ecb74bc",
   "metadata": {},
   "source": [
    "## Other fixture facts\n",
    "Once you get used to working with fixtures, they usually offer a more ergonomic and modular way of designing your testing suits than writing a lot of abstractions yourself.\n",
    "\n",
    "Let's look at another few things you can do with them:\n",
    "\n",
    "#### Fixtures can depend on fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65b7aa-6375-4e25-b6f7-bc8d78868d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "@pytest.fixture()\n",
    "def list_of_more_numbers(list_of_numbers):\n",
    "    return list_of_numbers.append([4, 5])\n",
    "\n",
    "def test_sum(list_of_more__numbers):\n",
    "    assert sum(list_of_more_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742bbae-1950-4325-ab45-51955a68bfe6",
   "metadata": {},
   "source": [
    "#### Testing functions can depend on multiple fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e548f64-303d-4269-be19-0c3ca7c80645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "@pytest.fixture()\n",
    "def more_numbers():\n",
    "    return [4, 5]\n",
    "\n",
    "def test_sum(list_of_numbers, more_numbers):\n",
    "    assert sum(list_of_more_numbers) + sum(more_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5636f2-0ea1-40e9-a8e3-a83b3be98122",
   "metadata": {},
   "source": [
    "# Pytest with pre-commit\n",
    "\n",
    "You can add pytest to pre-commit by adding a *local* hook to your `.pre-commit-config.yaml`.\n",
    "\n",
    "```yaml\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: pytest\n",
    "      name: pytest\n",
    "      entry: pytest\n",
    "      language: system\n",
    "      pass_filenames: false\n",
    "      always_run: true\n",
    "```\n",
    "\n",
    "However, running tests automatically before commits go through might be too intrusive, so be mindful of the trade-offs of installing this hook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
